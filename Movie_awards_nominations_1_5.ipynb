{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_awards_nominations_1_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFWBWfDptnjc",
        "outputId": "083dee6f-2a59-44ba-bc02-7a3aad35a5c5"
      },
      "source": [
        "pip install git+https://github.com/alberanid/imdbpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/alberanid/imdbpy\n",
            "  Cloning https://github.com/alberanid/imdbpy to /tmp/pip-req-build-htimjtmo\n",
            "  Running command git clone -q https://github.com/alberanid/imdbpy /tmp/pip-req-build-htimjtmo\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.7/dist-packages (from IMDbPY==2021.10.24) (1.4.25)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from IMDbPY==2021.10.24) (4.2.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy->IMDbPY==2021.10.24) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy->IMDbPY==2021.10.24) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy->IMDbPY==2021.10.24) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy->IMDbPY==2021.10.24) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIFQutooc287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cdedc0-1908-4e1c-c27b-b89b75c2c711"
      },
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "import difflib\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('stopwords')\n",
        "from imdb import IMDb\n",
        "import math\n",
        "from collections import Counter\n",
        "WORD = re.compile(r'\\w+')\n",
        "ia = IMDb()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcJUuRYIts4S",
        "outputId": "868ad4bf-ce33-4e49-8248-7cfdc907c8f1"
      },
      "source": [
        "mystop_words = stopwords.words('english')\n",
        "mystop_words.extend(['performance','made','-','congrats','congratulations','got','knows','well','well-','hell','yeah'])\n",
        "print(mystop_words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'performance', 'made', '-', 'congrats', 'congratulations', 'got', 'knows', 'well', 'well-', 'hell', 'yeah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqC2izeAZjZo"
      },
      "source": [
        "PEOPLE_AWARDS = ['best director - motion picture','best performance by an actress in a television series - comedy or musical','best performance by an actor in a supporting role in a motion picture','best performance by an actress in a supporting role in a series, mini-series or motion picture made for television','best performance by an actress in a motion picture - comedy or musical','best performance by an actress in a television series - drama','best performance by an actress in a motion picture - drama',\t'best performance by an actor in a motion picture - comedy or musical','best performance by an actor in a supporting role in a series, mini-series or motion picture made for television','best performance by an actress in a supporting role in a motion picture','best performance by an actor in a mini-series or motion picture made for television','best performance by an actress in a mini-series or motion picture made for television','best performance by an actor in a motion picture - drama', 'best performance by an actor in a television series - drama','best performance by an actor in a television series - comedy or musical']\n",
        "people_keywords = 'actor|actress|supporting|role|performance|director'\n",
        "series_keywords = 'series|tv|television'\n",
        "MOVIE_AWARDS = ['best screenplay - motion picture','best television series - comedy or musical','best original song - motion picture','best motion picture - comedy or musical','best foreign language film','best mini-series or motion picture made for television','best motion picture - drama','best television series - drama','best animated feature film','best original score - motion picture',]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHByIGMb0WON"
      },
      "source": [
        "with open('sample_data/movies2012.txt') as f:\n",
        "  movie_names = f.readlines()\n",
        "for i in range(len(movie_names)):\n",
        "  movie_names[i] = movie_names[i][:-1].lower()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNRHfNWDUbVn"
      },
      "source": [
        "def open_jsonfile(data_path = 'sample_data/gg2013.json'):\n",
        "  file_ = open(data_path)\n",
        "  data = json.load(file_)\n",
        "  return data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9737n42Jevr"
      },
      "source": [
        "def clean_award_name(award_name):\n",
        "  #removes stopwords from award name and returns key words in award name as list\n",
        "  award_name = award_name.lower()\n",
        "  cleaned_award = []\n",
        "  split = award_name.split()\n",
        "  for word in split:\n",
        "    if word not in mystop_words and word not in ['motion','picture']:  # Many words are irrelevant, removed with list of stopwords\n",
        "      cleaned_award.append(word)\n",
        "      if word == 'television':  # Usually television is shortened to tv, want to account for both\n",
        "        cleaned_award.append('tv')\n",
        "  return cleaned_award\n",
        "\n",
        "#print(clean_award_name(\"best motion picture - comedy or musical\"))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGsroAP2N8uF"
      },
      "source": [
        "def award_relevant_tweets(tweet_data, award_keywords):\n",
        "  relevant_tweets = []\n",
        "  for tweet in tweet_data:\n",
        "    text = tweet['text']\n",
        "    text = text.lower()\n",
        "    if 'rt @' in text:\n",
        "      continue\n",
        "    check = 0\n",
        "    for key_word in award_keywords:\n",
        "      if key_word not in text:\n",
        "        check +=1\n",
        "    if re.search(people_keywords,text):\n",
        "      continue\n",
        "    if re.search(series_keywords,text):\n",
        "      continue\n",
        "    if 'tv' in award_keywords:\n",
        "      if check <2:\n",
        "        relevant_tweets.append(tweet['text'].replace('-',' '))\n",
        "    else:\n",
        "      if check ==0:\n",
        "        relevant_tweets.append(tweet['text'].replace('-',' '))\n",
        "    \"\"\"else:\n",
        "      if winner.lower() in text:\n",
        "        relevant_tweets.append(tweet['text'])\"\"\"\n",
        "  return relevant_tweets\n",
        "\n",
        "######testing function###########\n",
        "#data = open_jsonfile(data_path = 'sample_data/gg2013.json')\n",
        "#award_keywords = clean_award_name('best performance by an actress in a motion picture - drama')\n",
        "#rel_tweets = award_relevant_tweets(data,award_keywords,winner = 'Jessica Chastain')\n",
        "#for rel_tweet in rel_tweets:\n",
        "#  print(rel_tweet)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sr14LHH7G9k"
      },
      "source": [
        "def remove_stop_words(small_tweet,mystop_words = mystop_words):\n",
        "  small_tweet = small_tweet.lower()\n",
        "  good_tweet = []\n",
        "  split = small_tweet.split()\n",
        "  for word in split:\n",
        "    if re.search('http',word.lower()):\n",
        "      continue\n",
        "    if word.lower() not in mystop_words:\n",
        "      good_tweet.append(word)\n",
        "  return(' '.join(good_tweet))\n",
        "\n",
        "###########testing function##########\n",
        "#small_tweet = 'Golden Globes: Winner for Best Supporting Actor in a Motion Picture is Christoph Waltz for \"Django Unchained\"'\n",
        "#print(remove_stop_words('small_tweet here is the link https://asfasdfsd.com'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EU_CEIfC0oS"
      },
      "source": [
        "def add_to_dictionary(potential_nominees, small_tweet, weightage):\n",
        "  #small_tweet = re.sub('#([a-zA-Z0-9_]{1,50})', '', small_tweet)\n",
        "  small_tweet = remove_stop_words(small_tweet)\n",
        "  if len(small_tweet.replace(' ','')) == 0:\n",
        "    return\n",
        "  small_tweet = small_tweet.replace('#',' ')\n",
        "  small_tweet = re.sub('@([a-zA-Z0-9_]{1,50})', '', small_tweet)\n",
        "  small_tweet = re.sub(r'[^\\w\\s]','',small_tweet)\n",
        "  if small_tweet in potential_nominees.keys():\n",
        "    potential_nominees[small_tweet] += weightage\n",
        "  else:\n",
        "    potential_nominees[small_tweet] = weightage"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fybgVS4VTOAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "bb487957-de7f-46c3-d35a-c36291e03a33"
      },
      "source": [
        "def populate_potential_movie_nominees(filtered_tweets):\n",
        "  stretch = 3\n",
        "  potential_nominees = {}\n",
        "  prior_keywords = ['won','win','winning','deserve','deserving','deserved','nomination','nominated','snubbed','better','is up for']\n",
        "  posterior_keywords = ['nominee','deserved award for','preferred','goes to','over','would have been']\n",
        "  biside_keywords = ['nom','beat','rob','beats','winner','beating','loser','losers','wins over']\n",
        "  negative_indicating_keywords = 'nominated|nominee|nomination|nominations'\n",
        "  negative_keywords = 'should have|did not|should\\'ve|not|reject'\n",
        "\n",
        "  for tweet in filtered_tweets:\n",
        "    text = tweet.lower()\n",
        "    if re.search(negative_indicating_keywords,text) and re.search(negative_keywords,text):\n",
        "      continue\n",
        "    for key_word in prior_keywords:\n",
        "      if key_word in text:\n",
        "        #if key_word == 'winning':\n",
        "        #  print(tweet)\n",
        "        small_tweet = tweet[:re.search(key_word,text).start()]\n",
        "        #small_tweet = remove_stop_words(small_tweet)\n",
        "        tokens = small_tweet.split()\n",
        "        if len(tokens) == 0:\n",
        "          continue\n",
        "        elif len(tokens) <= stretch:\n",
        "          small_tweet = ' '.join(tokens)\n",
        "          #small_tweet = remove_stop_words(small_tweet)\n",
        "          add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "        else:\n",
        "          small_tweet = ' '.join(tokens[-stretch::1])\n",
        "          #small_tweet = remove_stop_words(small_tweet)\n",
        "          add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "    for key_word in posterior_keywords:\n",
        "      if key_word in text:\n",
        "        small_tweet = tweet[re.search(key_word,text).start():]\n",
        "        #small_tweet = remove_stop_words(small_tweet)\n",
        "        tokens = small_tweet.split()\n",
        "        if len(tokens) <= 1:\n",
        "          continue\n",
        "        tokens = tokens[1::]\n",
        "        if len(tokens) <= stretch:\n",
        "          small_tweet = ' '.join(tokens)\n",
        "          #small_tweet = remove_stop_words(small_tweet)\n",
        "          add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "        else:\n",
        "          small_tweet = ' '.join(tokens[-stretch::1])\n",
        "          #small_tweet = remove_stop_words(small_tweet)\n",
        "          add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "    for key_word in biside_keywords:\n",
        "      if key_word in tweet.lower():\n",
        "        small_tweet_left = tweet[:re.search(key_word,text).start()]\n",
        "        small_tweet_right = tweet[re.search(key_word,text).start():]\n",
        "        #small_tweet_left = remove_stop_words(small_tweet_left)\n",
        "        #small_tweet_right = remove_stop_words(small_tweet_right)\n",
        "        tokens_left = small_tweet_left.split()\n",
        "        tokens_right = small_tweet_right.split()\n",
        "        if len(tokens_left) > 0:\n",
        "          if len(tokens_left) <= stretch:\n",
        "            small_tweet = ' '.join(tokens_left)\n",
        "            #small_tweet = remove_stop_words(small_tweet)\n",
        "            add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "          else:\n",
        "            small_tweet = ' '.join(tokens_left[-stretch::1])\n",
        "            #small_tweet = remove_stop_words(small_tweet)\n",
        "            add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "        if len(tokens_right) > 1:\n",
        "          tokens_right = tokens_right[1::]\n",
        "          if len(tokens_right) <= stretch:\n",
        "            small_tweet = ' '.join(tokens_right)\n",
        "            #small_tweet = remove_stop_words(small_tweet)\n",
        "            add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "          else:\n",
        "            small_tweet = ' '.join(tokens[-stretch::1])\n",
        "            #small_tweet = remove_stop_words(small_tweet)\n",
        "            add_to_dictionary(potential_nominees,small_tweet,weightage = 20)\n",
        "  return potential_nominees\n",
        "###################testing function#####################\n",
        "'''tweet_data = open_jsonfile(data_path = '/content/drive/MyDrive/NLP/gg2013.json')\n",
        "award_keywords = clean_award_name(\"best screenplay - motion picture\")\n",
        "relevant_tweets = award_relevant_tweets(tweet_data, award_keywords)\n",
        "potential_nominees = populate_potential_movie_nominees(relevant_tweets)\n",
        "for key in potential_nominees.keys():\n",
        "  print(key,potential_nominees[key])\n",
        "'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tweet_data = open_jsonfile(data_path = \\'/content/drive/MyDrive/NLP/gg2013.json\\')\\naward_keywords = clean_award_name(\"best screenplay - motion picture\")\\nrelevant_tweets = award_relevant_tweets(tweet_data, award_keywords)\\npotential_nominees = populate_potential_movie_nominees(relevant_tweets)\\nfor key in potential_nominees.keys():\\n  print(key,potential_nominees[key])\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCoXmHyhF0EV"
      },
      "source": [
        "def get_cosine(vec1, vec2):\n",
        "    # print vec1, vec2\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def text_to_vector(text):\n",
        "    return Counter(WORD.findall(text))\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    a = text_to_vector(a.strip().lower())\n",
        "    b = text_to_vector(b.strip().lower())\n",
        "\n",
        "    return get_cosine(a, b)\n",
        "\n",
        "##################testing function###############\n",
        "#print(cosine_similarity('steven ','steven speilberg'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmoJjftJstr"
      },
      "source": [
        "def char_similarity(a,b):\n",
        "  final_similarity = 0\n",
        "  a = a.lower()\n",
        "  b = b.lower()\n",
        "  a_tokens = a.split()\n",
        "  b_tokens = b.split()\n",
        "  for a_token in a_tokens:\n",
        "    for b_token in b_tokens:\n",
        "      similarity = 0\n",
        "      if len(a_token) > len(b_token):\n",
        "        bigger_token = a_token\n",
        "        smaller_token = b_token\n",
        "      else:\n",
        "        bigger_token = b_token\n",
        "        smaller_token = a_token\n",
        "      for i in range(len(smaller_token)):\n",
        "        if smaller_token[i] == bigger_token[i]:\n",
        "          similarity += 1\n",
        "      if final_similarity < similarity/len(smaller_token):\n",
        "        final_similarity = similarity/len(smaller_token)\n",
        "  return final_similarity\n",
        "\n",
        "##############testing function###################\n",
        "#print(char_similarity('ang lee','anger'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsqAIRo9-r8w"
      },
      "source": [
        "def merge_potential_nominees(potential_nominees):\n",
        "  compressed_nominees = {}\n",
        "  potential_nominees = sorted(potential_nominees.items(), key = lambda k: -k[1])\n",
        "  potential_nominees = dict(potential_nominees)\n",
        "  for key1 in potential_nominees.keys():\n",
        "    if len(compressed_nominees) == 0:\n",
        "      compressed_nominees[key1] = potential_nominees[key1]\n",
        "    else:\n",
        "      for key2 in list(compressed_nominees.keys()):\n",
        "        #if cosine_similarity(key1,key2) >= 0.6:\n",
        "        if len(difflib.get_close_matches(key1,[key2],cutoff = 0.6))==1:\n",
        "          compressed_nominees[key2] += potential_nominees[key1]\n",
        "          break\n",
        "        else:\n",
        "          if key2 == list(compressed_nominees.keys())[-1]:\n",
        "            compressed_nominees[key1] = potential_nominees[key1]\n",
        "  return compressed_nominees\n",
        "\n",
        "##############testing function###################\n",
        "#tweet_data = open_jsonfile(data_path = 'sample_data/gg2013.json')\n",
        "#award_keywords = clean_award_name(\"best supporting actress in a motion picture\")\n",
        "#relevant_tweets = award_relevant_tweets(tweet_data, award_keywords)\n",
        "#potential_nominees = populate_potential_movie_nominees(relevant_tweets)\n",
        "#merged_nominees = merge_potential_nominees(potential_nominees)\n",
        "#for key in merged_nominees.keys():\n",
        "#  print(key,merged_nominees[key])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5JNzhdPNysg"
      },
      "source": [
        "def check_on_imdb(compressed_nominees):\n",
        "  result_nominees = []\n",
        "  count = 0\n",
        "  for key in compressed_nominees.keys():\n",
        "    movies_identified = ia.search_person(key)\n",
        "\n",
        "    if len(movies_identified) == 0:\n",
        "      continue\n",
        "    if cosine_similarity(movies_identified[0]['name'],key) >= 0.6:\n",
        "      result_nominees.append(movies_identified[0]['name'])\n",
        "      count += 1\n",
        "    if count == 5 :\n",
        "      break\n",
        "  return result_nominees\n",
        "################testing function####################\n",
        "#result_nominees = check_on_imdb(merged_nominees)\n",
        "#print(result_nominees)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhqFidseyUgk"
      },
      "source": [
        "def check_on_movie_list(movie_names,merged_potential_nominees):\n",
        "  merged_potential_nominees = sorted(merged_potential_nominees.items(), key = lambda k: -k[1])\n",
        "  merged_potential_nominees = dict(merged_potential_nominees)\n",
        "  result_list = []\n",
        "  count = 0\n",
        "  #remove_list = []\n",
        "  for key in merged_potential_nominees.keys():\n",
        "    movies = difflib.get_close_matches(key,movie_names,cutoff = 0.6)\n",
        "    #print(key,movies)\n",
        "    if len(movies) != 0:\n",
        "      if (cosine_similarity(key,movies[0])+char_similarity(key,movies[0]))/2 >= 0.5:\n",
        "        if len(result_list) == 0:\n",
        "          result_list.append(movies[0])\n",
        "          #remove_list.append(key)\n",
        "          count+= 1          \n",
        "        elif movies[0] not in result_list:\n",
        "          result_list.append(movies[0])\n",
        "          #remove_list.append(key)\n",
        "          count+= 1\n",
        "    if count == 5:\n",
        "      break\n",
        "  '''if len(result_list) <5:\n",
        "      for key in merged_potential_nominees.keys():\n",
        "        if key in remove_list:\n",
        "          continue\n",
        "        movies = difflib.get_close_matches(key,movie_names,cutoff = 0.65)\n",
        "        #print(key,movies)\n",
        "        if len(movies) != 0:\n",
        "          if cosine_similarity(key,movies[0]) > 0.6:\n",
        "            result_list.append(movies[0])\n",
        "            remove_list.append(key)\n",
        "            count+= 1\n",
        "        if count == 5:\n",
        "          break'''\n",
        "  return result_list\n",
        "#print(difflib.get_close_matches('dark thirty shld',movie_names,cutoff = 0.6))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKSjneper9Gj"
      },
      "source": [
        "def find_nominees(award_name):\n",
        "  #open the json file of tweets\n",
        "  tweet_data = open_jsonfile()\n",
        "  #clean the award name and take it as list of tokens\n",
        "  award_keywords = clean_award_name(award_name)\n",
        "  #find all tweets relevant to this award name\n",
        "  filtered_tweets = award_relevant_tweets(tweet_data,award_keywords)\n",
        "  #for tweet in filtered_tweets:\n",
        "    #print(tweet)\n",
        "  #print(filtered_tweets)\n",
        "  #find potential nominees from relevant tweets\n",
        "  potential_nominees = populate_potential_movie_nominees(filtered_tweets)\n",
        "  #print(potential_nominees)\n",
        "  #merge potential nominees if there are various version of same nominees\n",
        "  merged_potential_nominees = merge_potential_nominees(potential_nominees)\n",
        "  #print(merged_potential_nominees)\n",
        "  #query imdb for best five results\n",
        "  #results = check_on_imdb(merged_potential_nominees)\n",
        "  results = check_on_movie_list(movie_names, merged_potential_nominees)\n",
        "  #return top five results\n",
        "  return results\n",
        "#winners = ['Ben Affleck','Lena Dunham']\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIsd64hfwspj",
        "outputId": "02a00f3f-d269-411d-b0ca-29607d8754e4"
      },
      "source": [
        "print(char_similarity('actofvalors','act of valor'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN3oAcOA6MqF",
        "outputId": "227cb12d-c376-4676-be07-ad6e2640a1c5"
      },
      "source": [
        "find_nominees('best motion picture - comedy or musical')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['les misã©rables', 'hors les murs', 'argo', 'fast girls', 'joker']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGl9Kxy1jX1w",
        "outputId": "c63e1d4e-ffba-40a4-e808-efba6f6e8aab"
      },
      "source": [
        "for movie_award in MOVIE_AWARDS:\n",
        "  print('-----------------------------')\n",
        "  print(movie_award)\n",
        "  print(find_nominees(movie_award))\n",
        "  print('-----------------------------')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "best screenplay - motion picture\n",
            "['django unchained', 'lincoln']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best television series - comedy or musical\n",
            "[]\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best original song - motion picture\n",
            "['skyfall', 'act of valor', 'picture day', 'the hunger games', 'les misã©rables']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best motion picture - comedy or musical\n",
            "['les misã©rables', 'hors les murs', 'argo', 'fast girls', 'joker']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best foreign language film\n",
            "['amour']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best mini-series or motion picture made for television\n",
            "[]\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best motion picture - drama\n",
            "['argo', 'lincoln', 'picture day', 'life of pi', 'zero dark thirty']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best television series - drama\n",
            "[]\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best animated feature film\n",
            "['brave', 'frankenweenie']\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "best original score - motion picture\n",
            "['life of pi', 'skyfall', 'this is 40', 'cloud atlas', 'argo']\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M9k1BIgvrNX"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agnLtvzGNmpU",
        "outputId": "3f254e91-3d14-4c7b-f22c-c8b9e6f1e654"
      },
      "source": [
        "tweet_data = open_jsonfile(data_path = 'sample_data/gg2013.json')\n",
        "#print(tweet_data[0])\n",
        "for data in tweet_data:\n",
        "  if 'salmon fishing' in data['text'].lower() and re.search('musical',data['text'].lower())and re.search('comedy',data['text'].lower()) and not re.search('actor|actress',data['text'].lower()) and not re.search('television|tv|series',data['text'].lower()):\n",
        "    print (data['text'].replace('#',''))\n",
        "    print()\n",
        "#award_keyword = clean_award_name(MOVIE_AWARDS[0])\n",
        "#award_relevant_tweets(tweet_data, award_keyword)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Musical/Comedy wins boost Oscar chances (no shit). Winners: Lawrence, Jackman, Les Miz. Losers: Salmon Fishing In The Yemen GoldenGlobes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7wC4GmcOsa8",
        "outputId": "722b33b5-b048-4218-99a8-fceb936709d0"
      },
      "source": [
        "small_tweet = 'WOW! QT for best screenplay on #DjangoUnchained. This room had #Lincoln winning'\n",
        "small_tweet = small_tweet.replace('#',' ')\n",
        "print(small_tweet)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WOW! QT for best screenplay on  DjangoUnchained. This room had  Lincoln winning\n"
          ]
        }
      ]
    }
  ]
}